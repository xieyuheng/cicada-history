#+title: parsing-macro

* feature

  #+begin_src rust
  
  #![feature (proc_macro_hygiene)]

  #![allow (unused_parens)]
  #![allow (dead_code)]
  #![allow (unused_macros)]
  #+end_src

* use

  #+begin_src rust
  extern crate proc_macro;

  use proc_macro::{
      TokenStream,
      TokenTree,
  };

  use quote::quote;
  #+end_src

* token_tree_vec_split

  #+begin_src rust
  fn token_tree_vec_split (
      token_tree_vec: Vec <TokenTree>,
      punct_ch: char,
  ) -> Vec <Vec <TokenTree>> {
      let mut result: Vec <Vec <TokenTree>> = Vec::new ();
      let mut vec: Vec <TokenTree> = Vec::new ();
      for token_tree in token_tree_vec .into_iter () {
          match token_tree {
              TokenTree::Punct (ref punct)
                  if punct_ch == punct.as_char () => {
                      result.push (vec);
                      vec = Vec::new ();
                  }
              _ => {
                  vec.push (token_tree);
              }
          }
      }
      // do not forget the last vec
      result.push (vec);
      result
  }
  #+end_src

* GrammarSyntaxError

  #+begin_src rust
  #[derive(Clone)]
  #[derive(Debug)]
  #[derive(PartialEq)]
  struct GrammarSyntaxError {
      report_message: String,
  }

  impl GrammarSyntaxError {
      fn new () -> Self {
          GrammarSyntaxError {
              report_message: format! ("- GrammarSyntaxError")
          }
      }

      fn report (&self) {
          eprintln! ("{}", self.report_message);
      }

      // line report
      fn lr (mut self, line: &str) -> Self {
          self.report_message = format! (
              "{}\n  {}",
              self.report_message,
              line);
          self
      }
  }
  #+end_src

* TokenExp

  #+begin_src rust
  #[derive(Clone)]
  #[derive(Debug)]
  #[derive(PartialEq)]
  enum TokenExp {
      Var (String),
      Str (String),
  }
  #+end_src

* RuleExp

  #+begin_src rust
  #[derive(Clone)]
  #[derive(Debug)]
  #[derive(PartialEq)]
  struct RuleExp {
      ante: Vec <TokenExp>,
      succ: Vec <Vec <TokenExp>>,
  }
  #+end_src

* string_string_p

  #+begin_src rust
  fn string_string_p (string: &str) -> bool {
      (string.len () >= 2 &&
       string.ends_with ('"') &&
       string.starts_with ('"'))
  }
  #+end_src

* string_string_to_string

  #+begin_src rust
  fn string_string_to_string (mut string: String) -> String {
      string.remove (0);
      string.pop ();
      string
  }
  #+end_src

* token_tree_vec_trans

  #+begin_src rust
  fn token_tree_vec_trans (
      token_tree_vec: Vec <TokenTree>
  ) -> Vec <TokenExp> {
      // ignore TokenTree unknown to this syntax, such as
      //   TokenTree::Punct
      //   TokenTree::Group
      //   TokenTree::Literal other then string
      let mut vec = Vec::new ();
      for token_tree in token_tree_vec {
          match token_tree {
              TokenTree::Ident (ident) => {
                  vec.push (TokenExp::Var (ident.to_string ()));
              }
              TokenTree::Literal (literal) => {
                  let repr = literal.to_string ();
                  if string_string_p (&repr) {
                      let string = string_string_to_string (repr);
                      vec.push (TokenExp::Str (string));
                  }
              }
              _ => {}
          }
      }
      vec
  }
  #+end_src

* parse_rule

  #+begin_src rust
  fn parse_rule (
      vec: Vec <TokenTree>,
  ) -> Result <RuleExp,
               GrammarSyntaxError> {
      // println! ("- parse_ruler");
      // println! ("  vec : {:?}", vec);
      let mut vec_vec = token_tree_vec_split (vec, '-');
      // println! ("  vec_vec : {:?}", vec_vec);
      if (vec_vec.len () != 2) {
          let error = GrammarSyntaxError::new ()
              .lr ("only one `-` in each rule")
              .lr (&format! ("found : {}", vec_vec.len () - 1));
          return Err (error)
      }
      let succ_vec = vec_vec.pop () .unwrap ();
      let ante_vec = vec_vec.pop () .unwrap ();
      let rule = RuleExp {
          ante: token_tree_vec_trans (ante_vec),
          succ: token_tree_vec_split (succ_vec, '|')
              .into_iter ()
              .map (token_tree_vec_trans)
              .collect ()
      };
      Ok (rule)
  }
  #+end_src

* parse_grammar

  #+begin_src rust
  fn parse_grammar (
      input: TokenStream,
  ) -> Result <Vec <RuleExp>,
               GrammarSyntaxError> {
      let mut result = Vec::new ();
      let token_tree_vec = input .into_iter () .collect ();
      let vec_vec = token_tree_vec_split (token_tree_vec, ';');
      // println! ("- parse_grammar");
      // println! ("  vec_vec : {:?}", vec_vec);
      for vec in vec_vec {
          if ! vec.is_empty () {
              // println! ("  vec : {:?}", vec);
              let rule = parse_rule (vec)?;
              result.push (rule);
          }
      }
      Ok (result)
  }
  #+end_src

* TokenExp::quote

  #+begin_src rust
  impl TokenExp {
      fn quote (self) -> proc_macro2::TokenStream {
          match self {
              TokenExp::Var (string) => {
                  quote! {
                      .var (#string)
                  }
              }
              TokenExp::Str (string) => {
                  quote! {
                      .chars (#string)
                  }
              }
          }
      }
  }
  #+end_src

* RuleExp::quote

  #+begin_src rust
  impl RuleExp {
      fn quote (self) -> proc_macro2::TokenStream {
          let mut output = quote! { };
          for token_exp in self.ante {
              output.extend (token_exp .quote ());
          }
          for vec in self.succ {
              output.extend (quote! {
                  .choice ()
              });
              for token_exp in vec {
                  output.extend (token_exp .quote ());
              }
          }
          output
      }
  }
  #+end_src

* grammar_fun

  #+begin_src rust
  fn grammar_fun (
      input: TokenStream,
  ) -> Result <TokenStream,
               GrammarSyntaxError> {
      // println! ("- input : {:#?}", input);
      let mut output = quote! {
          grammar_tech::Grammar::new ()
      };
      for rule_exp in parse_grammar (input)? .into_iter () {
          let rule_token_stream = rule_exp .quote ();
          output.extend (quote! {
              .rule (grammar_tech::Rule::new ()
                     #rule_token_stream)
          });
      }
      // println! ("- output : {:#?}", output);
      let output = TokenStream::from (output);
      Ok (output)
  }
  #+end_src

* grammar!

  #+begin_src rust
  #[proc_macro]
  pub fn grammar (input: TokenStream) -> TokenStream {
      match grammar_fun (input) {
          Ok (output) => output,
          Err (error) => {
              error.print ();
              panic! ("grammar macro fail");
          }
      }
  }
  #+end_src
